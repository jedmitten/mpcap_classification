{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s ; %(levelname)s ; %(message)s\", level=logging.INFO)\n",
    "logging.getLogger(\"scapy\").setLevel(logging.CRITICAL)\n",
    "logger = logging.getLogger(\"adAPT\")\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from time import perf_counter\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from math import log\n",
    "\n",
    "# import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# import numpy as np\n",
    "from scapy import all as sp\n",
    "from scapy.layers.http import HTTP, HTTPRequest, HTTPResponse\n",
    "from scapy.layers.dns import DNS\n",
    "\n",
    "from valid_tlds import TLDS\n",
    "\n",
    "sp.load_layer(\"http\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep pcaps\n",
    "perf_start = perf_counter()\n",
    "FILE_DIR = Path(\"~/GitRepos/challenge-datasets/\").expanduser()\n",
    "BENIGN_DIR = FILE_DIR / \"benign\"\n",
    "MALWAR_DIR = FILE_DIR / \"malware\"\n",
    "\n",
    "BENIGN_FILES = list([f for f in BENIGN_DIR.iterdir() if str(f).endswith(\".pcap\") or str(f).endswith(\".pcapng\")])\n",
    "MALWAR_FILES = list([f for f in MALWAR_DIR.iterdir() if str(f).endswith(\".pcap\") or str(f).endswith(\".pcapng\")])\n",
    "\n",
    "\n",
    "assert BENIGN_DIR.exists(), \"Benign dir cannot be found\"\n",
    "assert MALWAR_DIR.exists(), \"Malware dir cannot be found\"\n",
    "logger.debug(f\"Loaded files in {perf_counter() - perf_start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for skipping packets parsed by scapy\n",
    "EXCLUDE_NAMES = [\"Ethernet\", \"802.3\", \"cooked linux\", \"MPacket Preamble\"]\n",
    "INTERESTING_SERVICE_PORTS = [80, 443, 22, 53, 21, 20, 25, 465]\n",
    "IGNORE_SERVICE_PORTS = list(range(20))  # skip packets with ports lower than 20\n",
    "IGNORE_SERVICE_PORTS.append(37)  # time protocol\n",
    "IGNORE_SERVICE_PORTS + [67, 68]  # BOOTP protocol\n",
    "IGNORE_SERVICE_PORTS.append(123)  # NTP protocol\n",
    "IGNORE_SERVICE_PORTS.append(179)  # bgp protocol\n",
    "IGNORE_SERVICE_PORTS + [520, 521]  # RIP* protocol\n",
    "IGNORE_SERVICE_PORTS.append(646)  # ldp protocol\n",
    "IGNORE_SERVICE_PORTS.append(1967)  # CISCO IOS SLA protocol\n",
    "IGNORE_SERVICE_PORTS.append(1985)  # HSRP protocol\n",
    "IGNORE_SERVICE_PORTS + [5246, 5247]  # CAPWAP protocol\n",
    "\n",
    "\n",
    "# ensure no interesting ports are ignored\n",
    "IGNORE_SERVICE_PORTS = list(set(IGNORE_SERVICE_PORTS).difference(set(INTERESTING_SERVICE_PORTS)))\n",
    "\n",
    "\n",
    "class Protocol:\n",
    "    UDP = 17\n",
    "    IPv4 = 6\n",
    "    IPv6 = 34525\n",
    "    IPv6_enc = 41\n",
    "\n",
    "\n",
    "class App:\n",
    "    Unknown = \"Unknown\"\n",
    "    HTTP = \"HTTP\"\n",
    "    HTTPS = \"HTTPS\"\n",
    "    DNS = \"DNS\"\n",
    "    FTP = \"FTP\"\n",
    "    SSH = \"SSH\"\n",
    "    SMTP = \"SMTP\"\n",
    "    HTTPResponse = \"HTTPResponse\"\n",
    "    HTTPRequest = \"HTTPRequest\"\n",
    "    DNSQR = \"DNSQueryRequest\"\n",
    "    DNSRR = \"DNSRequestResponse\"\n",
    "\n",
    "\n",
    "HTTP_METHODS = [\"GET\", \"POST\", \"HEAD\", \"PUT\", \"DELETE\", \"CONNECT\", \"OPTIONS\", \"TRACE\", \"PATCH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derived feature functions\n",
    "def shannon(counts, thing):\n",
    "    frequencies = ((i / len(thing)) for i in counts.values())\n",
    "    return -sum(f * log(f, 2) for f in frequencies)\n",
    "\n",
    "\n",
    "def string_shannon(string):\n",
    "    counts = Counter(string)\n",
    "    return shannon(counts, string)\n",
    "\n",
    "\n",
    "def bytes_shannon(bytes):\n",
    "    counts = Counter(bytes)\n",
    "    return shannon(counts, bytes)\n",
    "\n",
    "\n",
    "def get_net_class(ip: str, class_type: str) -> Tuple[str]:\n",
    "    \"\"\"For ip = 192.168.1.5, provide the fillowing:\n",
    "    (\"192\", \"192.168\", \"192.168.1\", \"192.168.1.5\")\n",
    "    \"\"\"\n",
    "    parts = ip.split(\".\")\n",
    "    if len(parts) != 4:\n",
    "        return None\n",
    "    if class_type.lower() == \"a\":\n",
    "        return parts[0]\n",
    "    elif class_type.lower() == \"b\":\n",
    "        return \".\".join(parts[:2])\n",
    "    elif class_type.lower() == \"c\":\n",
    "        return \".\".join(parts[:3])\n",
    "    elif class_type.lower() == \"d\":\n",
    "        return ip\n",
    "    else:\n",
    "        raise ValueError(\"Class type must be A, B, C, or D\")\n",
    "\n",
    "\n",
    "def get_proto(pkt: sp.Packet) -> Any:\n",
    "    try:\n",
    "        if pkt.proto == Protocol.IPv6 or pkt.proto == Protocol.IPv6_enc:\n",
    "            return sp.IPv6\n",
    "        if pkt.proto == Protocol.IPv4:\n",
    "            return sp.IP\n",
    "        if pkt.proto == Protocol.UDP:\n",
    "            return sp.UDP\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def identify_layers(pkt: sp.Packet) -> App:\n",
    "    if HTTPRequest in pkt:\n",
    "        return App.HTTPRequest\n",
    "    elif HTTP in pkt or HTTPResponse in pkt:\n",
    "        return App.HTTPResponse\n",
    "    elif DNS in pkt:\n",
    "        if pkt[DNS].an:\n",
    "            return App.DNSRR\n",
    "        elif pkt[DNS].qd:\n",
    "            return App.DNSQR\n",
    "        return App.DNS\n",
    "    elif sp.Raw in pkt:\n",
    "        try:\n",
    "            lines = pkt.load.decode().split(\"\\n\")\n",
    "            logger.debug(f\"identifying raw from lines[0]: {lines[0]}\")\n",
    "            if lines[0].split(\" \")[0] in HTTP_METHODS:\n",
    "                return App.HTTPRequest\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    return App.Unknown\n",
    "\n",
    "\n",
    "def decode_if_able(d: Dict, key_filter: List[str] = None) -> Dict:\n",
    "    ALWAYS_FILTER = [\"flags\"]\n",
    "    if key_filter is None:\n",
    "        key_filter = []\n",
    "    outd = {}\n",
    "    for k, v in d.items():\n",
    "        if k in key_filter + ALWAYS_FILTER:\n",
    "            continue\n",
    "        k = k.lower()\n",
    "        try:\n",
    "            outd[k] = v.decode()\n",
    "        except:\n",
    "            outd[k] = v\n",
    "    return outd\n",
    "\n",
    "\n",
    "def gen_http_request_features(pkt: sp.Packet) -> Dict:\n",
    "    if sp.Raw in pkt:\n",
    "        raw_text = pkt[sp.Raw].load.decode()\n",
    "        lines = raw_text.split(\"\\n\")\n",
    "        method, path, http_version = [x.strip() for x in lines[0].split(\" \", maxsplit=3)]\n",
    "        _, host = [x.strip() for x in lines[1].split(\": \")]\n",
    "        _, user_agent = [x.strip() for x in lines[2].split(\": \")]\n",
    "        _, accept = [x.strip() for x in lines[3].split(\": \")]\n",
    "        _, accept_language = [x.strip() for x in lines[4].split(\": \")]\n",
    "        _, accept_encoding = [x.strip() for x in lines[5].split(\": \")]\n",
    "    else:\n",
    "        req = pkt[HTTPRequest]\n",
    "        method = req.Method.decode()\n",
    "        path = req.Path.decode()\n",
    "        host = req.Host.decode()\n",
    "        user_agent = req.User_Agent.decode()\n",
    "        accept = req.Accept.decode()\n",
    "        accept_language = req.Accept_Language.decode()\n",
    "        accept_encoding = req.Accept_Encoding.decode()\n",
    "        http_version = req.Http_Version.decode()\n",
    "\n",
    "    d = {\n",
    "        \"method\": method,\n",
    "        \"path\": path,\n",
    "        \"host\": host,\n",
    "        \"user_agent\": user_agent,\n",
    "        \"accept\": accept,\n",
    "        \"accept_language\": accept_language,\n",
    "        \"accept_encoding\": accept_encoding,\n",
    "        \"http_version\": http_version,\n",
    "    }\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def gen_http_response_features(pkt: sp.Packet) -> Dict:\n",
    "    resp = pkt[HTTPResponse]\n",
    "    raw_text = pkt[sp.Raw].load.decode()\n",
    "    d = decode_if_able(resp.fields)\n",
    "    d[\"raw_text\"] = raw_text\n",
    "    return d\n",
    "\n",
    "\n",
    "def gen_dns_request_features(pkt: sp.Packet) -> Dict:\n",
    "    qd = {}\n",
    "    if pkt[DNS].qd:\n",
    "        qd = decode_if_able(pkt[DNS].qd.fields)\n",
    "    return qd\n",
    "\n",
    "\n",
    "def gen_dns_response_features(pkt: sp.Packet) -> Dict:\n",
    "    qd = {}\n",
    "    an = {}\n",
    "    if pkt[DNS].qd:\n",
    "        qd = decode_if_able(pkt[DNS].qd.fields)\n",
    "    if pkt[DNS].an:\n",
    "        an = decode_if_able(pkt[DNS].an.fields)\n",
    "    qd.update(an)\n",
    "    return qd\n",
    "\n",
    "\n",
    "def parse_data(pkt: sp.Packet, app: App) -> Dict:\n",
    "    \"\"\"Turn a Raw payload into a dictionary of data\"\"\"\n",
    "    logger.debug(\"entering parse_raw...\")\n",
    "    if app != App.Unknown:\n",
    "        logger.debug(f\"Identified app: {app}\")\n",
    "\n",
    "    try:\n",
    "        if app == App.HTTPRequest:\n",
    "            return gen_http_request_features(pkt)\n",
    "        if app == App.HTTPResponse:\n",
    "            return gen_http_response_features(pkt)\n",
    "        if app == App.DNSQR:\n",
    "            return gen_dns_request_features(pkt)\n",
    "        if app == App.DNSRR:\n",
    "            return gen_dns_response_features(pkt)\n",
    "        # add other feature generators here\n",
    "        if sp.Raw in pkt:\n",
    "            # logger.warning(f\"Could not parse {pkt}\")\n",
    "            return {\"raw\": pkt[sp.Raw].load.decode()}\n",
    "    except Exception as e:\n",
    "        return {\"error\": \"could not parse packet data\"}\n",
    "\n",
    "\n",
    "def get_url(d: dict) -> str:\n",
    "    if d is None:\n",
    "        return None\n",
    "    url = None\n",
    "    if \"host\" in d:\n",
    "        url = d[\"host\"]\n",
    "    if \"qname\" in d:\n",
    "        url = d[\"qname\"]\n",
    "    if not isinstance(url, str):\n",
    "        return \"\"\n",
    "    if url.endswith(\".\"):\n",
    "        url = url[:-1]\n",
    "    return url\n",
    "\n",
    "\n",
    "def get_tld(s):\n",
    "    bd = get_base_domain(s)\n",
    "    if not bd:\n",
    "        return None\n",
    "    # split off the first part\n",
    "    parts = bd.split(\".\", maxsplit=1)\n",
    "    if len(parts) > 1:\n",
    "        parts.pop(0)\n",
    "        return parts[0]\n",
    "    return bd\n",
    "\n",
    "\n",
    "def get_base_domain(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    if not \".\" in s:\n",
    "        return \"\"\n",
    "    index = 0\n",
    "    for tld in TLDS:\n",
    "        if s.endswith(\".\" + tld):\n",
    "            tld_parts = tld.split(\".\")\n",
    "            index = len(tld_parts)\n",
    "            break\n",
    "    if not index:\n",
    "        return \"\"  # not a valid tld\n",
    "    index = index + 1\n",
    "    parts = s.rsplit(\".\", maxsplit=index)\n",
    "    return \".\".join(parts[-1 * index :])\n",
    "\n",
    "\n",
    "def get_host_part(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    if not \".\" in s:\n",
    "        return \"\"\n",
    "    base_domain = get_base_domain(s)\n",
    "    if base_domain:\n",
    "        tail_length = -1 * len(base_domain) - 1\n",
    "    else:\n",
    "        tail_length = len(s)\n",
    "    return s[:tail_length]  # extra -1 to account for trailing \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary method for creating and collecting features into DataFrames that will be pickled\n",
    "def make_rows(pkts: sp.PacketList) -> List[Dict]:\n",
    "    \"\"\"Read a packet, output a dict of values\"\"\"\n",
    "\n",
    "    logger.debug(f\"Filtering packets on IP, IPv6, and UDP\")\n",
    "\n",
    "    for pkt in pkts[sp.IP] + pkts[sp.IPv6] + pkts[sp.UDP]:\n",
    "        proto = get_proto(pkt)\n",
    "\n",
    "        logger.debug(f\"Identified proto as {proto}\")\n",
    "        try:\n",
    "            if sp.IP not in pkt:\n",
    "                logger.debug(\"Skipping packet without IP layer.\")\n",
    "                continue\n",
    "            try:\n",
    "                pkt[proto].sport\n",
    "                pkt[proto].dport\n",
    "                pkt[sp.IP].src\n",
    "                pkt[sp.IP].dst\n",
    "\n",
    "            except:\n",
    "                # this is not a packet with necessary attrs\n",
    "                # logger.warning(\"Could not find necessary attributes in packet, skipping...\")\n",
    "                continue\n",
    "            if pkt[proto].sport in IGNORE_SERVICE_PORTS or pkt[proto].dport in IGNORE_SERVICE_PORTS:\n",
    "                # skip packets with certain service ports\n",
    "                continue\n",
    "            parsed = None\n",
    "            layer = identify_layers(pkt)\n",
    "            if layer != App.Unknown:\n",
    "                logger.debug(f\"Found app layer {layer} in packet. Parsing...\")\n",
    "                parsed = parse_data(pkt, layer)\n",
    "            url = get_url(parsed)\n",
    "            tld = get_tld(url)\n",
    "            base_domain = get_base_domain(url)\n",
    "            host = get_host_part(url)\n",
    "            row = {\n",
    "                # I really want to figure out if there's a smart way to check packet rate between\n",
    "                # client and server but cannot think of it right now\n",
    "                # \"time\": pkt.time,\n",
    "                # \"day_hour\": datetime.fromtimestamp(pkt.time).strftime(\"%Y%m%d%H\"),\n",
    "                \"protocol\": pkt[proto].name,\n",
    "                \"app_layer\": layer,\n",
    "                \"source_addr\": pkt[sp.IP].src,\n",
    "                \"dest_addr\": pkt[sp.IP].dst,\n",
    "                \"source_port\": pkt[proto].sport,\n",
    "                \"dest_port\": pkt[proto].dport,\n",
    "                \"proto_packet_length\": pkt[proto].len,\n",
    "                \"proto_packet_cache\": pkt[proto].raw_packet_cache,\n",
    "                \"ip_packet_length\": pkt[proto].len,\n",
    "                \"ip_packet_cache\": pkt[sp.IP].raw_packet_cache,\n",
    "                \"parsed\": parsed,\n",
    "                \"url\": url,\n",
    "                \"tld\": tld,\n",
    "                \"base_domain\": base_domain,\n",
    "                \"host\": host,\n",
    "            }\n",
    "            logger.debug(f\"Yielding {row}...\")\n",
    "            yield row\n",
    "        except Exception as e:\n",
    "            # logger.exception(f\"Error running make_rows with pkt: {pkt}\")\n",
    "            continue\n",
    "    \n",
    "\n",
    "\n",
    "def main():\n",
    "    dfs = []\n",
    "    for fn in BENIGN_FILES:\n",
    "        with open(fn, \"rb\") as f:\n",
    "            logger.info(f\"Reading {fn}...\")\n",
    "            start_perf = perf_counter()\n",
    "            rows = list(make_rows(sp.rdpcap(f)))\n",
    "            logger.info(f\"Processing pcap took {perf_counter() - start_perf} seconds.\")\n",
    "            dfs.append(pd.DataFrame(rows))\n",
    "    benign_df = pd.concat(dfs)\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for fn in MALWAR_FILES:\n",
    "        with open(fn, \"rb\") as f:\n",
    "            logger.info(f\"Reading {fn}...\")\n",
    "            start_perf = perf_counter()\n",
    "            rows = list(make_rows(sp.rdpcap(f)))\n",
    "            logger.info(f\"Processing pcap took {perf_counter() - start_perf} seconds.\")\n",
    "            dfs.append(pd.DataFrame(rows))\n",
    "    malware_df = pd.concat(dfs)\n",
    "\n",
    "    logger.debug(f\"benign_df.shape: {benign_df.shape}\")\n",
    "    logger.debug(f\"malware_df.shape: {malware_df.shape}\")\n",
    "    \n",
    "    ### More features ###\n",
    "    for tmp_df in [malware_df, benign_df]:\n",
    "        tmp_df[\"url_entropy\"] = tmp_df.url.apply(lambda x: string_shannon(x) if x is not None else 0)\n",
    "        tmp_df[\"host_entropy\"] = tmp_df.host.apply(lambda x: string_shannon(x) if x is not None else 0)\n",
    "        tmp_df[\"base_domain_entropy\"] = tmp_df.base_domain.apply(lambda x: string_shannon(x) if x is not None else 0)\n",
    "        tmp_df[\"host_length\"] = tmp_df.host.apply(lambda x: len(x) if x is not None else 0)\n",
    "        tmp_df[\"proto_packet_entropy\"] = tmp_df.proto_packet_cache.apply(lambda x: bytes_shannon(x) if x is not None else 0)\n",
    "\n",
    "        tmp_df[\"source_ip_class_a\"] = tmp_df.source_addr.apply(lambda x: get_net_class(x, \"A\"))\n",
    "        tmp_df[\"source_ip_class_b\"] = tmp_df.source_addr.apply(lambda x: get_net_class(x, \"B\"))\n",
    "        tmp_df[\"source_ip_class_c\"] = tmp_df.source_addr.apply(lambda x: get_net_class(x, \"C\"))\n",
    "        tmp_df[\"dest_ip_class_a\"] = tmp_df.dest_addr.apply(lambda x: get_net_class(x, \"A\"))\n",
    "        tmp_df[\"dest_ip_class_b\"] = tmp_df.dest_addr.apply(lambda x: get_net_class(x, \"B\"))\n",
    "        tmp_df[\"dest_ip_class_c\"] = tmp_df.dest_addr.apply(lambda x: get_net_class(x, \"C\"))\n",
    "        \n",
    "    malware_df[\"malware\"] = 1.0\n",
    "    benign_df[\"malware\"] = 0.0\n",
    "        \n",
    "\n",
    "    b_pkl = \"./data/benign_features.pkl\"\n",
    "    m_pkl = \"./data/malicious_features.pkl\"\n",
    "    logger.info(f\"Writing output bengign dataframe (with {benign_df.shape[0]} rows) to: {b_pkl}\")\n",
    "    benign_df.to_pickle(b_pkl)\n",
    "    logger.info(f\"Writing output malicious dataframe (with {malware_df.shape[0]} rows) to: {m_pkl}\")\n",
    "    malware_df.to_pickle(m_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 17:34:20,861 ; INFO ; Reading /Users/jedmitten/GitRepos/challenge-datasets/benign/The Ultimate PCAP v20221220.pcapng...\n",
      "2023-05-14 17:34:43,288 ; INFO ; Processing pcap took 22.424999898000003 seconds.\n",
      "2023-05-14 17:34:43,395 ; INFO ; Reading /Users/jedmitten/GitRepos/challenge-datasets/malware/2023-03-07-Emotet-epoch4-infection-with-spambot-traffic-carved.pcap...\n",
      "2023-05-14 17:35:16,392 ; INFO ; Processing pcap took 32.99570774199999 seconds.\n",
      "2023-05-14 17:35:16,704 ; INFO ; Reading /Users/jedmitten/GitRepos/challenge-datasets/malware/2023-04-13-MetaStealer-C2-traffic.pcap...\n",
      "2023-05-14 17:35:26,433 ; INFO ; Processing pcap took 9.727458657999996 seconds.\n",
      "2023-05-14 17:35:26,512 ; INFO ; Reading /Users/jedmitten/GitRepos/challenge-datasets/malware/2023-01-05-Agent-Tesla-variant-traffic.pcap...\n",
      "2023-05-14 17:35:27,003 ; INFO ; Processing pcap took 0.4900021450000054 seconds.\n",
      "2023-05-14 17:35:27,011 ; INFO ; Reading /Users/jedmitten/GitRepos/challenge-datasets/malware/2023-01-16-IcedID-infection-with-Backonnect-and-VNC-and-Cobalt-Strike.pcap...\n",
      "2023-05-14 17:35:38,512 ; INFO ; Processing pcap took 11.500445913000007 seconds.\n",
      "2023-05-14 17:35:38,603 ; INFO ; Reading /Users/jedmitten/GitRepos/challenge-datasets/malware/2022-01-04-Remcos-RAT-infection-traffic.pcap...\n",
      "2023-05-14 17:35:41,322 ; INFO ; Processing pcap took 2.6679507800000124 seconds.\n",
      "2023-05-14 17:35:41,342 ; INFO ; Reading /Users/jedmitten/GitRepos/challenge-datasets/malware/2023-03-18-Emotet-E5-infection-traffic.pcap...\n",
      "2023-05-14 17:35:50,914 ; INFO ; Processing pcap took 9.570543366999999 seconds.\n",
      "2023-05-14 17:35:50,980 ; INFO ; Reading /Users/jedmitten/GitRepos/challenge-datasets/malware/2022-12-07-Bumblebee-infection-with-Cobalt-Strike.pcap...\n",
      "2023-05-14 17:35:54,743 ; INFO ; Processing pcap took 3.7620345429999986 seconds.\n",
      "2023-05-14 17:35:54,762 ; INFO ; Reading /Users/jedmitten/GitRepos/challenge-datasets/malware/2022-09-21-Astaroth-Guildma-infection.pcap...\n",
      "2023-05-14 17:36:00,868 ; INFO ; Processing pcap took 6.104917408999995 seconds.\n",
      "2023-05-14 17:36:04,020 ; INFO ; Writing output bengign dataframe (with 29959 rows) to: ./data/benign_features.pkl\n",
      "2023-05-14 17:36:04,106 ; INFO ; Writing output malicious dataframe (with 167022 rows) to: ./data/malicious_features.pkl\n"
     ]
    }
   ],
   "source": [
    "# Run the feature generation and output the pkl files\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
